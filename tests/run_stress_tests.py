#!/usr/bin/env python3
# coding=utf-8
"""
MPMSå‹åŠ›æµ‹è¯•è¿è¡Œè„šæœ¬
æä¾›ä¸åŒçº§åˆ«çš„æµ‹è¯•é€‰é¡¹å’Œè¯¦ç»†æŠ¥å‘Š
"""

import os
import sys
import time
import subprocess
import argparse
import logging
from pathlib import Path
import json
from typing import Dict, List, Any

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# æµ‹è¯•é…ç½®
STRESS_TEST_CONFIGS = {
    'quick': {
        'description': 'å¿«é€Ÿå‹åŠ›æµ‹è¯•ï¼ˆé€‚åˆå¼€å‘é˜¶æ®µï¼‰',
        'files': ['test_stress_comprehensive.py::TestMPMSStress::test_edge_cases',
                 'test_performance_benchmark.py::TestMPMSPerformance::test_baseline_performance'],
        'timeout': 300,  # 5åˆ†é’Ÿ
    },
    'standard': {
        'description': 'æ ‡å‡†å‹åŠ›æµ‹è¯•ï¼ˆé€‚åˆCI/CDï¼‰',
        'files': ['test_stress_comprehensive.py',
                 'test_performance_benchmark.py::TestMPMSPerformance::test_baseline_performance',
                 'test_performance_benchmark.py::TestMPMSPerformance::test_scaling_performance'],
        'timeout': 1800,  # 30åˆ†é’Ÿ
    },
    'intensive': {
        'description': 'å¯†é›†å‹åŠ›æµ‹è¯•ï¼ˆé€‚åˆå‘å¸ƒå‰ï¼‰',
        'files': ['test_stress_comprehensive.py',
                 'test_performance_benchmark.py',
                 'test_extreme_scenarios.py'],
        'timeout': 3600,  # 1å°æ—¶
    },
    'extreme': {
        'description': 'æç«¯å‹åŠ›æµ‹è¯•ï¼ˆé€‚åˆé•¿æœŸç¨³å®šæ€§æµ‹è¯•ï¼‰',
        'files': ['test_stress_comprehensive.py',
                 'test_performance_benchmark.py',
                 'test_extreme_scenarios.py'],
        'timeout': 7200,  # 2å°æ—¶
        'extra_args': ['--stress-multiplier=2']
    }
}


class StressTestRunner:
    """å‹åŠ›æµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self, test_level: str = 'standard'):
        self.test_level = test_level
        self.config = STRESS_TEST_CONFIGS[test_level]
        self.results = {}
        self.start_time = None
        self.end_time = None
        
        # ç¡®ä¿åœ¨testsç›®å½•ä¸‹è¿è¡Œ
        self.tests_dir = Path(__file__).parent
        os.chdir(self.tests_dir)
        
        logger.info(f"åˆå§‹åŒ–å‹åŠ›æµ‹è¯•è¿è¡Œå™¨ - çº§åˆ«: {test_level}")
        logger.info(f"é…ç½®: {self.config['description']}")
        
    def run_single_test(self, test_file: str, timeout: int = None) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªæµ‹è¯•æ–‡ä»¶"""
        logger.info(f"å¼€å§‹è¿è¡Œæµ‹è¯•: {test_file}")
        
        # æ„å»ºå‘½ä»¤ï¼Œå¿½ç•¥pytest.iniä¸­çš„coverageé…ç½®
        cmd = [
            'timeout', f'{timeout or self.config["timeout"]}s',
            'bash', '-c', 
            f'cd {self.tests_dir} && python -m pytest {test_file} -v -s --tb=short --override-ini="addopts=-ra --strict-markers --tb=short"'
        ]
        
        # æ·»åŠ é¢å¤–å‚æ•°
        if 'extra_args' in self.config:
            cmd[-1] += ' ' + ' '.join(self.config['extra_args'])
        
        start_time = time.time()
        
        try:
            # è¿è¡Œæµ‹è¯•
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=timeout or self.config['timeout']
            )
            
            end_time = time.time()
            duration = end_time - start_time
            
            test_result = {
                'test_file': test_file,
                'duration': duration,
                'return_code': result.returncode,
                'stdout': result.stdout,
                'stderr': result.stderr,
                'success': result.returncode == 0,
                'timeout_occurred': False
            }
            
            if result.returncode == 0:
                logger.info(f"âœ… æµ‹è¯•é€šè¿‡: {test_file} ({duration:.1f}ç§’)")
            else:
                logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {test_file} ({duration:.1f}ç§’)")
                logger.error(f"é”™è¯¯è¾“å‡º: {result.stderr[:500]}...")
                
        except subprocess.TimeoutExpired:
            end_time = time.time()
            duration = end_time - start_time
            
            test_result = {
                'test_file': test_file,
                'duration': duration,
                'return_code': -1,
                'stdout': '',
                'stderr': f'æµ‹è¯•è¶…æ—¶ ({timeout}ç§’)',
                'success': False,
                'timeout_occurred': True
            }
            
            logger.error(f"â° æµ‹è¯•è¶…æ—¶: {test_file} ({duration:.1f}ç§’)")
            
        except Exception as e:
            end_time = time.time()
            duration = end_time - start_time
            
            test_result = {
                'test_file': test_file,
                'duration': duration,
                'return_code': -2,
                'stdout': '',
                'stderr': str(e),
                'success': False,
                'timeout_occurred': False
            }
            
            logger.error(f"ğŸ’¥ æµ‹è¯•å¼‚å¸¸: {test_file} - {e}")
        
        return test_result
    
    def run_all_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        logger.info(f"å¼€å§‹è¿è¡Œ {self.test_level} çº§åˆ«çš„å‹åŠ›æµ‹è¯•")
        logger.info(f"é¢„è®¡è¿è¡Œæ—¶é—´: {self.config['timeout']} ç§’")
        
        self.start_time = time.time()
        
        # è¿è¡Œæ¯ä¸ªæµ‹è¯•æ–‡ä»¶
        for test_file in self.config['files']:
            self.results[test_file] = self.run_single_test(test_file)
            
            # å¦‚æœæ˜¯ä¸¥é‡å¤±è´¥ï¼Œè€ƒè™‘æ˜¯å¦ç»§ç»­
            if not self.results[test_file]['success'] and self.results[test_file]['return_code'] < 0:
                logger.warning(f"ä¸¥é‡å¤±è´¥ï¼Œç»§ç»­ä¸‹ä¸€ä¸ªæµ‹è¯•...")
        
        self.end_time = time.time()
        
        # ç”ŸæˆæŠ¥å‘Š
        return self.generate_report()
    
    def generate_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        total_duration = self.end_time - self.start_time
        successful_tests = sum(1 for r in self.results.values() if r['success'])
        total_tests = len(self.results)
        
        report = {
            'test_level': self.test_level,
            'total_duration': total_duration,
            'total_tests': total_tests,
            'successful_tests': successful_tests,
            'failed_tests': total_tests - successful_tests,
            'success_rate': successful_tests / total_tests if total_tests > 0 else 0,
            'test_results': self.results,
            'summary': self._generate_summary()
        }
        
        return report
    
    def _generate_summary(self) -> str:
        """ç”Ÿæˆæµ‹è¯•æ‘˜è¦"""
        total_tests = len(self.results)
        successful_tests = sum(1 for r in self.results.values() if r['success'])
        failed_tests = total_tests - successful_tests
        total_duration = self.end_time - self.start_time
        
        summary_lines = [
            f"å‹åŠ›æµ‹è¯•æ‘˜è¦ - {self.test_level.upper()} çº§åˆ«",
            "=" * 50,
            f"æ€»æµ‹è¯•æ•°: {total_tests}",
            f"æˆåŠŸ: {successful_tests}",
            f"å¤±è´¥: {failed_tests}",
            f"æˆåŠŸç‡: {(successful_tests/total_tests*100):.1f}%" if total_tests > 0 else "æˆåŠŸç‡: N/A",
            f"æ€»è€—æ—¶: {total_duration:.1f} ç§’",
            "",
            "è¯¦ç»†ç»“æœ:"
        ]
        
        for test_file, result in self.results.items():
            status = "âœ…" if result['success'] else "âŒ"
            duration = result['duration']
            summary_lines.append(f"  {status} {test_file} ({duration:.1f}s)")
            
            if not result['success']:
                error_msg = result['stderr'][:100] + "..." if len(result['stderr']) > 100 else result['stderr']
                summary_lines.append(f"      é”™è¯¯: {error_msg}")
        
        return "\n".join(summary_lines)
    
    def save_report(self, report: Dict[str, Any], output_file: str = None) -> str:
        """ä¿å­˜æµ‹è¯•æŠ¥å‘Š"""
        if output_file is None:
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            output_file = f"stress_test_report_{self.test_level}_{timestamp}.json"
        
        output_path = self.tests_dir / output_file
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
        return str(output_path)
    
    def print_summary(self, report: Dict[str, Any]):
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        print("\n" + report['summary'])
        
        # é¢å¤–çš„ç»Ÿè®¡ä¿¡æ¯
        if report['failed_tests'] > 0:
            print(f"\nâš ï¸  æœ‰ {report['failed_tests']} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œè¯·æŸ¥çœ‹è¯¦ç»†æ—¥å¿—")
        
        if report['success_rate'] >= 0.9:
            print("\nğŸ‰ å‹åŠ›æµ‹è¯•æ•´ä½“è¡¨ç°ä¼˜ç§€ï¼")
        elif report['success_rate'] >= 0.7:
            print("\nğŸ‘ å‹åŠ›æµ‹è¯•è¡¨ç°è‰¯å¥½")
        elif report['success_rate'] >= 0.5:
            print("\nâš ï¸  å‹åŠ›æµ‹è¯•è¡¨ç°ä¸€èˆ¬ï¼Œéœ€è¦å…³æ³¨")
        else:
            print("\nğŸš¨ å‹åŠ›æµ‹è¯•è¡¨ç°è¾ƒå·®ï¼Œéœ€è¦ç´§æ€¥å¤„ç†")


def check_dependencies():
    """æ£€æŸ¥ä¾èµ–"""
    missing_deps = []
    
    try:
        import psutil
    except ImportError:
        missing_deps.append('psutil')
    
    try:
        import pytest
    except ImportError:
        missing_deps.append('pytest')
    
    if missing_deps:
        logger.error(f"ç¼ºå°‘ä¾èµ–: {', '.join(missing_deps)}")
        logger.error("è¯·è¿è¡Œ: pip install " + " ".join(missing_deps))
        sys.exit(1)


def main():
    parser = argparse.ArgumentParser(description='MPMSå‹åŠ›æµ‹è¯•è¿è¡Œå™¨')
    parser.add_argument(
        '--level', 
        choices=['quick', 'standard', 'intensive', 'extreme'],
        default='standard',
        help='æµ‹è¯•çº§åˆ« (é»˜è®¤: standard)'
    )
    parser.add_argument(
        '--output', 
        help='æŠ¥å‘Šè¾“å‡ºæ–‡ä»¶å'
    )
    parser.add_argument(
        '--list-levels', 
        action='store_true',
        help='åˆ—å‡ºæ‰€æœ‰æµ‹è¯•çº§åˆ«'
    )
    parser.add_argument(
        '--dry-run', 
        action='store_true',
        help='åªæ˜¾ç¤ºå°†è¦è¿è¡Œçš„æµ‹è¯•ï¼Œä¸å®é™…æ‰§è¡Œ'
    )
    
    args = parser.parse_args()
    
    if args.list_levels:
        print("å¯ç”¨çš„æµ‹è¯•çº§åˆ«:")
        for level, config in STRESS_TEST_CONFIGS.items():
            print(f"  {level}: {config['description']}")
            print(f"    é¢„è®¡æ—¶é—´: {config['timeout']} ç§’")
            print(f"    æµ‹è¯•æ–‡ä»¶: {', '.join(config['files'])}")
            print()
        return
    
    # æ£€æŸ¥ä¾èµ–
    check_dependencies()
    
    # åˆ›å»ºè¿è¡Œå™¨
    runner = StressTestRunner(args.level)
    
    if args.dry_run:
        print(f"å°†è¦è¿è¡Œçš„æµ‹è¯• ({args.level} çº§åˆ«):")
        for test_file in runner.config['files']:
            print(f"  - {test_file}")
        print(f"é¢„è®¡æ€»æ—¶é—´: {runner.config['timeout']} ç§’")
        return
    
    try:
        # è¿è¡Œæµ‹è¯•
        report = runner.run_all_tests()
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = runner.save_report(report, args.output)
        
        # æ˜¾ç¤ºæ‘˜è¦
        runner.print_summary(report)
        
        # è¿”å›é€‚å½“çš„é€€å‡ºç 
        if report['success_rate'] >= 0.7:
            sys.exit(0)
        else:
            sys.exit(1)
            
    except KeyboardInterrupt:
        logger.info("æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        logger.error(f"è¿è¡Œæµ‹è¯•æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main() 